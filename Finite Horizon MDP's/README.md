# FINITE MARKOV DECISION PROCESS AND BELLMAN EQUATIONS

Finite Markov Decision Processes (MDPs) have been used to formulate many decision-making problems in science and engineering. The objective is to synthesize the best decision (action selection) policies to maximize expected rewards (minimize costs) in a given stochastic dynamical environment.  It involve majorly two algorithms Value Iteration and Policy Iteration.

The resources for the topic are

[Sutton and Barto chapter 3](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)

[Dr Balaraman Ravindran's lectures on NPTEL week 3 and week 4](https://nptel.ac.in/courses/106106143/) 

[Articles on Medium](https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690)

I have implemented the following algorithms on  **frozenlake-v0** environment

- [x] Policy Iteration
- [x] Value Iteration

I have attached my hackmd notes of this topic below. It has the intuitions to all concepts as well as the mathematical proof for its existence.

[Check out my notes on Finite Markov Decision Process and Bellman Equations.](https://hackmd.io/K61TBYpxQ1yXPXjm5TbbwQ?both)





